import operator
import random
import numpy as np
from deap import base, creator, tools, gp
import networkx as nx
from deap.gp import graph
from networkx.drawing.nx_agraph import graphviz_layout
from collections import OrderedDict
# import matplotlib.pyplot as plt
from liu.DatasetReader import createNode
from Class.Taskflow import TaskFlow
from liu.exetime import computing_Task, computing_upload_time
import multiprocessing
import copy
import json
import os

#  ç»Ÿä¸€å‚æ•°è®¾ç½®
NUM_NODES = 5                 # èŠ‚ç‚¹æ•°
NUM_TASKFLOWS = 10             # ä»»åŠ¡æµæ•°
POP_SIZE = 10                  # ç§ç¾¤è§„æ¨¡
NGEN = 2                       # è¿›åŒ–ä»£æ•°
CXPB = 0.8                     # äº¤å‰æ¦‚ç‡
MUTPB = 0.1                    # å˜å¼‚æ¦‚ç‡
TOURNAMENT_SIZE = 1           # é”¦æ ‡èµ›é€‰æ‹©è§„æ¨¡
HEIHT_LIMIT = 6               # æœ€å¤§æ ‘é«˜
NUM_TREES = 2                 # æ¯ä¸ªä¸ªä½“ä¸­æ ‘çš„æ•°é‡
NUM_RUNS = 1                # é‡å¤è¿è¡Œæ¬¡æ•°ï¼ˆç”¨äºç»Ÿè®¡å‡å€¼ï¼‰
ELITISM_NUM = int(POP_SIZE * 0.05)  # ç²¾è‹±ä¸ªä½“çš„æ•°é‡
NUM_TEST_SETS = 1             # æµ‹è¯•é›†æ•°é‡
NUM_TRAIN_SETS = 1             # è®­ç»ƒé›†æ•°é‡
BASE_SEED = 10000              # è®­ç»ƒé›†æˆ‘ä»¬ä»10000å¼€å§‹

def initIndividual(container, func, pset, size):
    return container(gp.PrimitiveTree(func(pset[i])) for i in range(size))

def cxOnePointListOfTrees(ind1, ind2):
    print("type:", type(ind1))
    for idx, (tree1, tree2) in enumerate(zip(ind1, ind2)):
        print(f"\n===== äº¤å‰å¤„ç†ç¬¬ {idx} æ£µå­æ ‘ =====")
        print("ğŸ”µ åŸå§‹ tree1 (str):", str(tree1))
        print("ğŸ”µ åŸå§‹ tree2 (str):", str(tree2))

        HEIGHT_LIMIT = 8
        dec = gp.staticLimit(key=operator.attrgetter("height"), max_value=HEIGHT_LIMIT)
        tree1, tree2 = dec(gp.cxOnePoint)(tree1, tree2)
        print("ğŸŸ¢ äº¤å‰å tree1_new (str):", str(tree1))
        print("ğŸŸ¢ äº¤å‰å tree2_new (str):", str(tree2))
        ind1[idx], ind2[idx] = tree1, tree2

    return ind1, ind2

def mutUniformListOfTrees(individual, expr, pset):
    for idx, tree in enumerate(individual):
        HEIGHT_LIMIT = 8
        dec = gp.staticLimit(key=operator.attrgetter("height"), max_value=HEIGHT_LIMIT)
        tree, = dec(gp.mutUniform)(tree, expr=expr,pset=pset[idx])
        individual[idx] = tree

    return (individual,)

def varAnd(population, toolbox, cxpb, mutpb):
    offspring = [toolbox.clone(ind) for ind in population]
    evlpb=cxpb/(cxpb+mutpb)
    if random.random() < evlpb:
        for i in range(1, len(offspring), 2):
            offspring[i - 1], offspring[i] = toolbox.mate(offspring[i - 1],
                                                          offspring[i])

            del offspring[i - 1].fitness.values, offspring[i].fitness.values

    else:
        for i in range(len(offspring)):
            offspring[i], = toolbox.mutate(offspring[i])
            del offspring[i].fitness.values

    return offspring

def keyadd(p,x):
    all_keys = set(p.keys()).union(x.keys())
    result = OrderedDict()
    for key in all_keys:
        result[key] = p.get(key, 0) + x.get(key, 0)
    return result

def sortPopulation(toolbox, population):
    # å…‹éš†ä¸ªä½“ï¼Œé¿å…ä¿®æ”¹åŸå§‹ç§ç¾¤
    populationCopy = [toolbox.clone(ind) for ind in population]
    # ä½¿ç”¨ sorted æŒ‰é€‚åº”åº¦å‡åºæ’åº
    sorted_population = sorted(populationCopy, key=lambda ind: ind.fitness.values)
    return sorted_population

def is_number(string):#åˆ›å»ºä¸€ä¸ªå‡½æ•° is_numberï¼Œç”¨äºæ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦å¯ä»¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚å¦‚æœå¯ä»¥ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    try:
        float(string)
        return True
    except ValueError:
        return False

def count_leaf_types(tree):
    leaf_count={}
    name_mapping = {
        'ARG0': 'a', 'ARG1': 'b', 'ARG2': 'c',
        'ARG3': 'd', 'ARG4': 'e', 'ARG5': 'f'
    }
    """ç»Ÿè®¡æ¯ç§å¶å­èŠ‚ç‚¹çš„æ•°é‡"""
    for x in tree:
        if isinstance(x, gp.Terminal):  # å¦‚æœæ˜¯å¶å­èŠ‚ç‚¹
            leaf_name = x.name
            if leaf_name in name_mapping:
                leaf_name = name_mapping[leaf_name]
            leaf_count[leaf_name] = leaf_count.get(leaf_name, 0) + 1
    return leaf_count

def protect_div(left, right):
    with np.errstate(divide='ignore', invalid='ignore'):
        x = np.divide(left, right)
        if isinstance(x, np.ndarray):
            x[np.isinf(x)] = 1
            x[np.isnan(x)] = 1
        elif np.isinf(x) or np.isnan(x):
            x = 1
    return x

def record_best_individual_log(individual, pre_generated_taskflows, nodes, pset, generation_index, run_index, base_dir="best_ind_logs"):

    test_taskflows_sample = copy.deepcopy(pre_generated_taskflows[0])
    nodes_log = copy.deepcopy(nodes)

    _, log_dict = work_processing(individual, test_taskflows_sample, nodes_log, pset, return_log=True)

    log_dict["individual_expressions"] = [str(tree) for tree in individual]

    run_dir = os.path.join(base_dir, f"run_{run_index}")
    os.makedirs(run_dir, exist_ok=True)

    log_path = os.path.join(run_dir, f"gen_{generation_index}_log.json")
    with open(log_path, "w", encoding='utf-8') as f:
        json.dump(log_dict, f, indent=2, ensure_ascii=False)

def get_fifo_node_score(node):
    """è¿”å› FIFO èŠ‚ç‚¹ä¼˜å…ˆçº§å¾—åˆ†ï¼šèŠ‚ç‚¹ç©ºé—²æ—¶é—´è¶Šæ—©å¾—åˆ†è¶Šé«˜"""
    return -node.begin_idle_time

def get_fifo_task_score(task):
    """è¿”å› FIFO ä»»åŠ¡ä¼˜å…ˆçº§å¾—åˆ†ï¼šåˆ°è¾¾æ—¶é—´è¶Šæ—©å¾—åˆ†è¶Šé«˜"""
    return -task.arrivetime

def allocate_resources(node, task):
    node.cpu_capacity -= task.cpu_require
    node.ram_capacity -= task.ram_require
    node.gpu_capacity -= task.gpu_require

def release_resources(node, task):
    node.cpu_capacity += task.cpu_require
    node.ram_capacity += task.ram_require
    node.gpu_capacity += task.gpu_require

def createTaskFlows(num_TaskFlow,genre,seed): #åˆ›å»ºå¤šä¸ªå·¥ä½œæµ #kä¸ºä»»åŠ¡æµéšæœºç§å­
    lambda_rate = 1  # å¹³å‡åˆ°è¾¾é€Ÿç‡ (Î»ï¼Œæ¯å•ä½æ—¶é—´å¹³å‡åˆ°è¾¾ä»»åŠ¡æ•°)
    np.random.seed(seed)
    interarrival_times = np.random.exponential(1 / lambda_rate, num_TaskFlow)  # ä¼šç”Ÿæˆä¸€ä¸ªåŒ…å« num_tasks ä¸ªä»»åŠ¡åˆ°è¾¾æ—¶é—´é—´éš”çš„æ•°ç»„

    # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„åˆ°è¾¾æ—¶é—´ï¼ˆé€šè¿‡ç´¯åŠ é—´éš”æ—¶é—´ï¼‰
    arrival_times = np.cumsum(interarrival_times)
    taskflows=[TaskFlow(id,arrival_time,genre,id+(seed)*num_TaskFlow) for id, arrival_time in zip(range(num_TaskFlow), arrival_times)]
    return taskflows

def present_time_update(present_time,taskflows):#åªæ›´æ–°é‚£äº›æœªå®Œæˆä»»åŠ¡çš„å½“å‰æ—¶é—´ï¼Œæ‰€ä»¥å½“ä¸€ä¸ªä»»åŠ¡å·²å®Œæˆæ—¶ï¼Œå®ƒçš„å½“å‰æ—¶é—´å°±æ˜¯å®Œæˆæ—¶é—´
    for taskflow in taskflows:
        for task in taskflow.tasks:
            if task.finish is False:
                task.present_time=present_time

def find_earlist_time(queue1,queue2):
    # ä½¿ç”¨ sort() æ–¹æ³•æ’åº
    queue1.sort(key=lambda x: x[1])
    queue2.sort(key=lambda x: x[1])
    task_queue1 = []
    task_queue2 = []
    if len(queue1)!=0 and len(queue2)!=0:
        min_time=min(queue1[0][1],queue2[0][1])
        for event in queue1:
            if event[1]==min_time:
                task_queue1.append(event[0])
            elif event[1] > min_time:
                break
        for event in queue2:
            if event[1]==min_time:
                task_queue2.append(event[0])
            elif event[1] > min_time:
                break
    elif len(queue1)==0  and len(queue2)!=0:
        min_time = queue2[0][1]
        for event in queue2:
            if event[1]==min_time:
                task_queue2.append(event[0])
            elif event[1] > min_time:
                break
    elif len(queue1)!=0  and len(queue2)==0:
        min_time = queue1[0][1]
        for event in queue1:
            if event[1]==min_time:
                task_queue1.append(event[0])
            elif event[1] > min_time:
                break
    return queue1,queue2,task_queue1,task_queue2

# æ·»åŠ æ–°æ–¹æ³•

def decode1(individual, task, nodes, taskflows, pset):
    try:
        heuristic_1 = gp.compile(expr=individual[0], pset=pset[0])
        scores = []
        for node in nodes:
            if (node.cpu_available >= task.cpu_require and node.ram_available >= task.ram_require and
                node.gpu_available >= task.gpu_require):
                score = heuristic_1(computing_Task(task, node), task.present_time - task.arrivetime,
                    len(task.descendant), computing_upload_time(task, node), get_fifo_node_score(node))
                scores.append((node, score))
        if not scores:
            return None
        return max(scores, key=lambda x: x[1])[0]
    except Exception as e:
        print(f"[âŒ å¼‚å¸¸] decode1 å¤„ç†ä»»åŠ¡ {task.global_id} æ—¶å‡ºé”™ï¼š{e}")
        return None

def decode2(individual, node, taskflows, nodes, pset):
    try:
        heuristic_2 = gp.compile(expr=individual[1], pset=pset[1])
        scores = []
        for task in node.waiting_queue:
            if (node.cpu_available >= task.cpu_require and node.ram_available >= task.ram_require and
                node.gpu_available >= task.gpu_require):
                k = task.taskflow_id
                score = heuristic_2(computing_Task(task, node), task.present_time - task.arrivetime, len(task.descendant),
                    taskflows[k].find_descendant_avg_time(taskflows, task, nodes),
                    0.1 * computing_upload_time(task, node), get_fifo_task_score(task))
                scores.append((task, score))
        if not scores:
            return None
        return max(scores, key=lambda x: x[1])[0]
    except Exception as e:
        print(f"[âŒ å¼‚å¸¸] decode2 é€‰æ‹©èŠ‚ç‚¹ {node.id} çš„ç­‰å¾…ä»»åŠ¡æ—¶å‡ºé”™ï¼š{e}")
        return None

def evaluate_offspring_in_parallel(offspring, taskflows, nodes, pset, work_processing):
    pool = multiprocessing.Pool()
    results = []

    for ind in offspring:
        tasks_copy = copy.deepcopy(taskflows)
        nodes_copy = copy.deepcopy(nodes)
        result = pool.apply_async(work_processing, (ind, tasks_copy, nodes_copy, pset,))
        results.append(result)

    pool.close()
    pool.join()

    fitnesses = [res.get() for res in results]
    return fitnesses

def evaluate_on_testSets(individual, nodes, pset, pre_generated_taskflows, return_log=False):
    results = []
    logs = []

    for i, taskflows in enumerate(pre_generated_taskflows):
        nodes_copy = copy.deepcopy(nodes)
        taskflows_copy = copy.deepcopy(taskflows)

        if return_log:
            fitness, log_data = work_processing(individual, taskflows_copy, nodes_copy, pset, return_log=True)
            logs.append({
                "test_set_index": i,
                "fitness": fitness,
                "log": log_data
            })
        else:
            fitness = work_processing(individual, taskflows_copy, nodes_copy, pset)
        results.append(fitness[0])  # fitness is a tuple: (avg_time,)

    if return_log:
        return results, logs
    else:
        return results

def eaSimple(population, toolbox, nodes, pre_generated_taskflows, num_TaskFlow,
             cxpb, mutpb, ngen, elitism, pset, num_run, min_fitness_values=None, genre_min_fitness_values=None):
    if min_fitness_values is None:
        min_fitness_values = []
    if genre_min_fitness_values is None:
        genre_min_fitness_values = []

    # åˆå§‹ç§ç¾¤è¯„ä¼°ï¼ˆè¿™é‡Œä¹Ÿæ˜¯è¦å¯¹è®­ç»ƒé›†è¿›è¡Œæ”¹é€ çš„ï¼‰
    initial_inds = [ind for ind in population if not ind.fitness.valid]
    initial_taskflows_list = [
        createTaskFlows(num_TaskFlow, 0, BASE_SEED + i)
        for i in range(NUM_TRAIN_SETS)
    ]
    fitnesses_list = [
        evaluate_offspring_in_parallel(initial_inds, taskflows, nodes, pset, work_processing)
        for taskflows in initial_taskflows_list
    ]

    fitnesses_avg = [
        (np.mean([fitnesses_list[run][i][0] for run in range(NUM_TRAIN_SETS)]),)
        for i in range(len(initial_inds))
    ]

    for ind, fit in zip(initial_inds, fitnesses_avg):
        ind.fitness.values = fit

    elite_inds = sortPopulation(toolbox, population)[:elitism]

    # æµ‹è¯•é›†è¯„ä¼°æœ€ä¼˜ä¸ªä½“
    test_fitnesses = evaluate_on_testSets(elite_inds[0], nodes, pset, pre_generated_taskflows)
    genre_min_fitness_values.append(sum(test_fitnesses) / NUM_TEST_SETS)

    # ä¿å­˜ç¬¬0ä»£æœ€ä¼˜ä¸ªä½“è°ƒåº¦æ—¥å¿—å’Œè¡¨è¾¾å¼
    record_best_individual_log(elite_inds[0],pre_generated_taskflows,nodes,pset,0,num_run)

    for gen in range(1, ngen + 1):

        # æ¯ä¸€è½®è®­ç»ƒçš„æ—¶å€™æ”¹ä¸ºä¸¤æ¬¡ä»¿çœŸ
        # train_taskflows = createTaskFlows(num_TaskFlow, 0, gen)
        train_taskflows_list = [
            createTaskFlows(num_TaskFlow, 0, BASE_SEED + gen * NUM_TRAIN_SETS + i)
            for i in range(NUM_TRAIN_SETS)
        ]

        # é€‰æ‹© + å˜å¼‚ + äº¤å‰
        offspring_inds = toolbox.select(population, len(population) - elitism)
        offspring_inds = varAnd(offspring_inds, toolbox, cxpb, mutpb)
        offspring_inds[:] = elite_inds + offspring_inds

        # è®­ç»ƒé›†è¯„ä¼°
        fitnesses_list = []
        for train_taskflows in train_taskflows_list:
            fitnesses = evaluate_offspring_in_parallel(offspring_inds, train_taskflows, nodes, pset, work_processing)
            fitnesses_list.append(fitnesses)
        # ä¸¤æ¬¡ä»¿çœŸè¯„ä¼°å¹¶å–å¹³å‡é€‚åº”åº¦
        fitnesses_avg = []
        for i in range(len(offspring_inds)):
            avg_fit = np.mean([fitnesses_list[run][i][0] for run in range(NUM_TRAIN_SETS)])
            fitnesses_avg.append((avg_fit,))

        # æ›´æ–°ä¸ªä½“é€‚åº”åº¦
        for ind, fit in zip(offspring_inds, fitnesses_avg):
            ind.fitness.values = fit

        population[:] = offspring_inds

        elite_inds = sortPopulation(toolbox, population)[:elitism]

        # æµ‹è¯•é›†è¯„ä¼°æœ€ä¼˜ä¸ªä½“
        test_fitnesses = evaluate_on_testSets(elite_inds[0], nodes, pset, pre_generated_taskflows)
        genre_min_fitness_values.append(sum(test_fitnesses) / NUM_TEST_SETS)

        # ä¿å­˜å½“å‰ä»£æœ€ä¼˜ä¸ªä½“è°ƒåº¦æ—¥å¿—å’Œè¡¨è¾¾å¼
        record_best_individual_log(elite_inds[0],pre_generated_taskflows,nodes,pset,gen,num_run)

    return population, genre_min_fitness_values, elite_inds[0]

def work_processing(individual, taskflows, nodes, pset, return_log=False):
    def sanitize_task_id(task):
        return getattr(task, "global_id", f"Task {task.id}")

    def sanitize_node_id(node):
        return f"N{node.id}({node.node_type})"

    task_execution_log = []
    node_assignment_log = {}
    taskflow_summary_log = []
    skipped_tasks_log = []

    queue1 = []  # æœªæ‰§è¡Œä»»åŠ¡é˜Ÿåˆ—
    queue2 = []  # æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡é˜Ÿåˆ—
    present_time = 0
    present_time_update(present_time, taskflows)

    print("ğŸš€ [è°ƒåº¦å¼€å§‹] æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦æµç¨‹å¯åŠ¨...\n")

    for taskflow in taskflows:
        tasks = taskflow.find_predecessor_is_zero()
        for task in tasks:
            queue1.append((task, task.arrivetime))

    while queue1 or queue2:
        queue1, queue2, task_queue1, task_queue2 = find_earlist_time(queue1, queue2)

        if task_queue1:
            current_time = queue1[0][1]
            present_time_update(present_time=current_time, taskflows=taskflows)
            print(f"\nâ° æ—¶é—´æ¨è¿›è‡³ {current_time:.2f}ï¼Œå¤„ç†é˜Ÿåˆ— queue1 ä¸­çš„ä»»åŠ¡ï¼š")

            for task in task_queue1:
                node = decode1(individual, task, nodes, taskflows, pset)
                task.node = node
                print(f"ğŸŸ¡ ä»»åŠ¡ {sanitize_task_id(task)} åˆ†é…è‡³èŠ‚ç‚¹ {sanitize_node_id(node)}")

                if task.present_time >= node.begin_idle_time:
                    try:
                        allocate_resources(task, node)
                    except Exception as e:
                        print(f"[âŒèµ„æºåˆ†é…å¤±è´¥] Task {task.global_id} åˆ†é…åˆ° Node {node.id} æ—¶å¤±è´¥ï¼š{e}")
                        skipped_tasks_log.append({
                            "task_id": sanitize_task_id(task),
                            "taskflow_id": task.taskflow_id,
                            "reason": f"èµ„æºåˆ†é…å¤±è´¥: {str(e)}",
                            "node_id": sanitize_node_id(node),
                            "present_time": task.present_time
                        })
                        continue

                    task_time = computing_Task(task, node) + computing_upload_time(task, node)
                    endtime = task.present_time + task_time
                    task.endtime = endtime
                    queue2.append((task, endtime))
                    node.begin_idle_time = endtime

                    # âœ… è®°å½•æˆåŠŸè°ƒåº¦çš„ä»»åŠ¡
                    task_execution_log.append({
                        "task_id": sanitize_task_id(task),
                        "taskflow_id": task.taskflow_id,
                        "node_id": sanitize_node_id(node),
                        "start_time": task.present_time,
                        "end_time": task.endtime
                    })

                    print(f"âœ… æ‰§è¡Œä»»åŠ¡ {sanitize_task_id(task)}ï¼Œé¢„è®¡å®Œæˆæ—¶é—´ä¸º {endtime:.2f}")
                else:
                    node.waiting_queue.append(task)
                    print(f"â³ èŠ‚ç‚¹å¿™ï¼Œä»»åŠ¡ {sanitize_task_id(task)} åŠ å…¥ç­‰å¾…é˜Ÿåˆ—")

                if sanitize_node_id(node) not in node_assignment_log:
                    node_assignment_log[sanitize_node_id(node)] = []
                node_assignment_log[sanitize_node_id(node)].append(sanitize_task_id(task))

                queue1 = [item for item in queue1 if item[0] != task]

        if task_queue2:
            current_time = queue2[0][1]
            present_time_update(present_time=current_time, taskflows=taskflows)
            print(f"\nğŸ æ—¶é—´æ¨è¿›è‡³ {current_time:.2f}ï¼Œå¤„ç†å®Œæˆçš„ä»»åŠ¡ï¼š")

            for finish_event in task_queue2:
                finish_event.finish = True
                release_resources(finish_event, finish_event.node)
                print(f"âœ”ï¸ ä»»åŠ¡ {sanitize_task_id(finish_event)} æ‰§è¡Œå®Œæˆ")

            for task in task_queue2:
                taskflow = taskflows[task.taskflow_id]

                if task.descendant:
                    current_index = taskflow.tasks.index(task)
                    descendant_tasks = []
                    for d in task.descendant:
                        taskflow.tasks[d].predecessor.remove(current_index)
                        if len(taskflow.tasks[d].predecessor) == 0:
                            descendant_tasks.append(taskflow.tasks[d])
                    for descendant_task in descendant_tasks:
                        queue1.append((descendant_task, descendant_task.present_time))
                        print(f"â¡ï¸ åç»§ä»»åŠ¡ {sanitize_task_id(descendant_task)} æ‰€æœ‰å‰é©±å®Œæˆï¼ŒåŠ å…¥ queue1")
                else:
                    taskflow.finish_time = max(taskflow.finish_time, current_time)
                    print(f"ğŸ ä»»åŠ¡æµ {task.taskflow_id} æ›´æ–°å®Œæˆæ—¶é—´ä¸º {taskflow.finish_time:.2f}")

                if task.node.waiting_queue:
                    next_task = decode2(individual, task.node, taskflows, nodes, pset)
                    task.node.waiting_queue.remove(next_task)
                    trans_delay = 0.1 * computing_upload_time(task, task.node)
                    task_time = computing_Task(next_task, task.node) + computing_upload_time(next_task, task.node) + trans_delay
                    next_task.endtime = next_task.present_time + task_time
                    queue2.append((next_task, next_task.endtime))
                    task.node.begin_idle_time = next_task.endtime

                    # âœ… è®°å½• decode2 åˆ†é…çš„ä»»åŠ¡
                    task_execution_log.append({
                        "task_id": sanitize_task_id(next_task),
                        "taskflow_id": next_task.taskflow_id,
                        "node_id": sanitize_node_id(task.node),
                        "start_time": next_task.present_time,
                        "end_time": next_task.endtime
                    })

                    print(f"ğŸ“¤ èŠ‚ç‚¹ {sanitize_node_id(task.node)} æ‰§è¡Œç­‰å¾…ä»»åŠ¡ {sanitize_task_id(next_task)}ï¼Œå®Œæˆæ—¶é—´ä¸º {next_task.endtime:.2f}")

                queue2 = [item for item in queue2 if item[0] != task]

    print("\nğŸ“¦ [ä»»åŠ¡åˆ†é…ç»Ÿè®¡] å„èŠ‚ç‚¹æ‰§è¡Œçš„ä»»åŠ¡åˆ†å¸ƒå¦‚ä¸‹ï¼š")
    for node in nodes:
        executed_tasks = node_assignment_log.get(sanitize_node_id(node), [])
        if executed_tasks:
            print(f"ğŸ“Œ èŠ‚ç‚¹ {sanitize_node_id(node)} æ‰§è¡Œä»»åŠ¡ï¼š{', '.join(executed_tasks)}")
        else:
            print(f"ğŸ“Œ èŠ‚ç‚¹ {sanitize_node_id(node)} æœªæ‰§è¡Œä»»ä½•ä»»åŠ¡")

    print("\nğŸ“Š [è°ƒåº¦å®Œæˆ] å¼€å§‹ç»Ÿè®¡ä»»åŠ¡æµå®Œæˆæ—¶é—´ï¼š")
    sum_time = 0
    count = 0
    for tf in taskflows[10:]:
        duration = tf.finish_time - tf.all_arrive_time
        taskflow_summary_log.append({
            "taskflow_id": tf.id,
            "start_time": tf.all_arrive_time,
            "end_time": tf.finish_time,
            "duration": duration
        })
        print(f"ğŸ“˜ TaskFlow {tf.id}ï¼šå¼€å§‹ {tf.all_arrive_time:.2f}ï¼Œå®Œæˆ {tf.finish_time:.2f}ï¼Œè€—æ—¶ {duration:.2f}")
        sum_time += duration
        count += 1

    avg_time = sum_time / count if count > 0 else 0
    print(f"\nğŸ“ˆ å 10 ä¸ªä»»åŠ¡æµçš„å¹³å‡å®Œæˆæ—¶é—´ä¸ºï¼š{avg_time:.2f}\n")

    if return_log:
        log_data = {
            "avg_time": avg_time,
            "task_execution_log": task_execution_log,
            "node_assignment_log": node_assignment_log,
            "taskflow_summary_log": taskflow_summary_log,
            "skipped_tasks_log": skipped_tasks_log
        }
        return avg_time, log_data
    else:
        return (avg_time,)

def main_dual_tree(num_run):
    nodes = createNode(NUM_NODES,0)
    # taskflows=createTaskFlows(NUM_TASKFLOWS,0,0)
    pre_generated_taskflows = []
    for i in range(NUM_TEST_SETS):
        taskflow = createTaskFlows(NUM_TASKFLOWS,1,i)
        pre_generated_taskflows.append(taskflow)
    pset = []
    for idx in range(2):
        if idx == 0:
            pset1 = gp.PrimitiveSet("MAIN", 4)
            pset1.addPrimitive(operator.add, 2)
            pset1.addPrimitive(operator.sub, 2)
            pset1.addPrimitive(operator.mul, 2)
            pset1.addPrimitive(protect_div, 2)
            pset1.addPrimitive(operator.neg, 1)
            pset1.addPrimitive(max, 2)
            pset1.addPrimitive(min, 2)
            pset1.renameArguments(ARG0='TCT')  # ä»»åŠ¡çš„è®¡ç®—æ—¶é—´ tct
            pset1.renameArguments(ARG1='TWT')  # ä»»åŠ¡çš„ç­‰å¾…æ—¶é—´=ä»»åŠ¡çš„åˆ°è¾¾æ—¶é—´-ä»»åŠ¡çš„å½“å‰æ—¶é—´ twt
            pset1.renameArguments(ARG2='TNC')  # ä»»åŠ¡çš„åç»§æ•°é‡ tnc
            pset1.renameArguments(ARG3='TUT')  # ä»»åŠ¡çš„ä¸Šä¼ æ—¶é—´  tut
            pset1.renameArguments(ARG4='FIFO1')
            pset.append(pset1)

        elif idx == 1:
            pset2 = gp.PrimitiveSet("MAIN", 5)
            pset2.addPrimitive(operator.add, 2)
            pset2.addPrimitive(operator.sub, 2)
            pset2.addPrimitive(operator.mul, 2)
            pset2.addPrimitive(protect_div, 2)
            pset2.addPrimitive(operator.neg, 1)
            pset2.addPrimitive(max, 2)
            pset2.addPrimitive(min, 2)
            pset2.renameArguments(ARG0='TCT')  # ä»»åŠ¡çš„è®¡ç®—æ—¶é—´
            pset2.renameArguments(ARG1='TWT')  # ä»»åŠ¡çš„ç­‰å¾…æ—¶é—´=ä»»åŠ¡çš„åˆ°è¾¾æ—¶é—´-ä»»åŠ¡çš„å½“å‰æ—¶é—´
            pset2.renameArguments(ARG2='TNC')  # ä»»åŠ¡çš„åç»§æ•°é‡
            pset2.renameArguments(ARG3='TPS')  # ä»»åŠ¡çš„ç»™åç»§ä¼ é€’æ¶ˆæ¯çš„æ—¶é—´ tps
            pset2.renameArguments(ARG4='ACT')  # åç»§èŠ‚ç‚¹çš„å¹³å‡å®Œæˆæ—¶é—´ act
            pset2.renameArguments(ARG5='FIFO2')
            pset.append(pset2)

    # åˆ›å»ºä¸€ä¸ªé€‚åº”åº¦ç±»å’Œä¸ªä½“ç±»ï¼Œä¸ªä½“ç”±å¤šæ£µæ ‘ç»„æˆ
    creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
    creator.create("Individual", list, fitness=creator.FitnessMin)
    toolbox = base.Toolbox()
    toolbox.register("expr", gp.genHalfAndHalf, min_=2, max_=4)
    toolbox.register("individual", initIndividual, creator.Individual, toolbox.expr, pset=pset, size=NUM_TREES)  # å‡è®¾æˆ‘ä»¬åˆ›å»º2ä¸ªæ ‘
    toolbox.register("population", tools.initRepeat, list, toolbox.individual)
    toolbox.register("evaluate", work_processing)#ä¸èƒ½ç»‘å®štaskflows=taskflows,nodes=nodes
    toolbox.register("select", tools.selTournament, tournsize=TOURNAMENT_SIZE)
    toolbox.register("mate", cxOnePointListOfTrees)
    toolbox.register("mutate", mutUniformListOfTrees, expr=toolbox.expr, pset=pset)
    toolbox.register("compile", gp.compile)

    population = toolbox.population(n=POP_SIZE)
    pop,genre_min_fitness_values,elite= eaSimple(population=population,
                                                       toolbox=toolbox,
                                                       # taskflows=taskflows,
                                                       nodes=nodes,
                                                       pre_generated_taskflows=pre_generated_taskflows,
                                                       num_TaskFlow=NUM_TASKFLOWS,
                                                       cxpb=CXPB,
                                                       mutpb=MUTPB,
                                                       ngen=NGEN,
                                                       elitism=ELITISM_NUM,
                                                       pset=pset,
                                                       num_run = num_run,
                                                       min_fitness_values=[],
                                                       genre_min_fitness_values=[]
                                                 )
    leaf_ratio_result=[]
    for tree in elite:
        # ç»Ÿè®¡æ¯ç§ç»ˆç«¯çš„æ•°é‡
        leaf_count = count_leaf_types(tree)
        # è®¡ç®—æ ‘ä¸­æ‰€æœ‰ç»ˆç«¯çš„æ€»æ•°
        total_leaves = sum(leaf_count.values())
        # è®¡ç®—æ¯ç§ç»ˆç«¯çš„æ¯”ä¾‹ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨OrderedDictä¸­
        leaf_ratio = OrderedDict((key, value / total_leaves) for key, value in leaf_count.items())
        leaf_ratio_result.append(leaf_ratio)
    return genre_min_fitness_values,leaf_ratio_result

if __name__ == '__main__':

    run_fitness_history = []
    # è®°å½•æ¯ä¸€ä»£çš„æµ‹è¯•é›†æœ€å°é€‚åº”åº¦çš„ç´¯åŠ å€¼
    genre_min_fitness_values_sum = [0] * NGEN
    # è®°å½•æ¯æ£µæ ‘çš„å¶å­ç±»å‹ç»Ÿè®¡æ¯”ä¾‹ç´¯åŠ å€¼
    leaf_ratio_result_sum = [OrderedDict() for _ in range(NUM_TREES)]
    # NUM_RUNS æ¬¡ç‹¬ç«‹è¿è¡Œ
    for num_run in range(NUM_RUNS):
        # æ¯æ¬¡ç‹¬ç«‹è¿è¡Œåï¼Œå¾—åˆ°æ¯ä¸€ä»£çš„æµ‹è¯•é›†çš„æœ€å°é€‚åº”åº¦ï¼Œä»¥åŠæ¯æ£µæ ‘çš„å¶å­æ¯”ä¾‹ç»Ÿè®¡
        genre_min_fitness_values, leaf_ratio_result = main_dual_tree(num_run)

        run_fitness_history.append(genre_min_fitness_values)
        genre_min_fitness_values_sum = [a + b for a, b in
                                        zip(genre_min_fitness_values, genre_min_fitness_values_sum)]
        leaf_ratio_result_sum = [keyadd(a, b) for a, b in zip(leaf_ratio_result, leaf_ratio_result_sum)]

    genre_min_fitness_values_sum = [a / NUM_RUNS for a in genre_min_fitness_values_sum]



