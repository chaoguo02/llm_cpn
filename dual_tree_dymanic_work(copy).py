import operator
import random
import numpy as np
from deap import base, creator, tools, gp
import networkx as nx
from deap.gp import graph
from networkx.drawing.nx_agraph import graphviz_layout
from collections import OrderedDict

from core_function.data_loader import createNode
from core_function.update_time import computing_Task, computing_upload_time
# import matplotlib.pyplot as plt
from entity.Taskflow import TaskFlow
import multiprocessing
import copy
import json
import os

#  ç»Ÿä¸€å‚æ•°è®¾ç½®
NUM_NODES = 10                 # èŠ‚ç‚¹æ•°
NUM_TASKFLOWS = 50             # ä»»åŠ¡æµæ•°
POP_SIZE = 100                  # ç§ç¾¤è§„æ¨¡
NGEN = 50                       # è¿›åŒ–ä»£æ•°
CXPB = 0.8                     # äº¤å‰æ¦‚ç‡
MUTPB = 0.1                    # å˜å¼‚æ¦‚ç‡
TOURNAMENT_SIZE = 5           # é”¦æ ‡èµ›é€‰æ‹©è§„æ¨¡
HEIHT_LIMIT = 8               # æœ€å¤§æ ‘é«˜
NUM_TREES = 2                 # æ¯ä¸ªä¸ªä½“ä¸­æ ‘çš„æ•°é‡
NUM_RUNS = 1                # é‡å¤è¿è¡Œæ¬¡æ•°ï¼ˆç”¨äºç»Ÿè®¡å‡å€¼ï¼‰
ELITISM_NUM = int(POP_SIZE * 0.05)  # ç²¾è‹±ä¸ªä½“çš„æ•°é‡
NUM_TEST_SETS = 30             # æµ‹è¯•é›†æ•°é‡
NUM_TRAIN_SETS = 2             # è®­ç»ƒé›†æ•°é‡
BASE_SEED = 10000              # è®­ç»ƒé›†æˆ‘ä»¬ä»10000å¼€å§‹

def initIndividual(container, func, pset, size):
    return container(gp.PrimitiveTree(func(pset[i])) for i in range(size))

def cxOnePointListOfTrees(ind1, ind2):
    print("type:", type(ind1))
    for idx, (tree1, tree2) in enumerate(zip(ind1, ind2)):
        print(f"\n===== äº¤å‰å¤„ç†ç¬¬ {idx} æ£µå­æ ‘ =====")
        print("ğŸ”µ åŸå§‹ tree1 (str):", str(tree1))
        print("ğŸ”µ åŸå§‹ tree2 (str):", str(tree2))

        HEIGHT_LIMIT = 8
        dec = gp.staticLimit(key=operator.attrgetter("height"), max_value=HEIGHT_LIMIT)
        tree1, tree2 = dec(gp.cxOnePoint)(tree1, tree2)
        print("ğŸŸ¢ äº¤å‰å tree1_new (str):", str(tree1))
        print("ğŸŸ¢ äº¤å‰å tree2_new (str):", str(tree2))
        ind1[idx], ind2[idx] = tree1, tree2

    return ind1, ind2

def mutUniformListOfTrees(individual, expr, pset):
    for idx, tree in enumerate(individual):
        HEIGHT_LIMIT = 8
        dec = gp.staticLimit(key=operator.attrgetter("height"), max_value=HEIGHT_LIMIT)
        tree, = dec(gp.mutUniform)(tree, expr=expr,pset=pset[idx])
        individual[idx] = tree

    return (individual,)

def varAnd(population, toolbox, cxpb, mutpb):
    offspring = [toolbox.clone(ind) for ind in population]
    evlpb=cxpb/(cxpb+mutpb)
    if random.random() < evlpb:
        for i in range(1, len(offspring), 2):
            offspring[i - 1], offspring[i] = toolbox.mate(offspring[i - 1],
                                                          offspring[i])

            del offspring[i - 1].fitness.values, offspring[i].fitness.values

    else:
        for i in range(len(offspring)):
            offspring[i], = toolbox.mutate(offspring[i])
            del offspring[i].fitness.values

    return offspring

def sortPopulation(toolbox, population):#å‡½æ•° sortPopulation å¯¹è¾“å…¥çš„ç§ç¾¤è¿›è¡Œæ’åºï¼ˆæŒ‰ç…§é€‚åº”åº¦,å†’æ³¡æ’åºï¼‰ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ’åºåçš„ç§ç¾¤å‰¯æœ¬ï¼Œè€Œä¸ä¼šä¿®æ”¹åŸå§‹çš„population
    populationCopy = [toolbox.clone(ind) for ind in population]
    popsize = len(population)
    for j in range(popsize):
        sign = False
        for i in range(popsize-1-j):
            sum_fit_i = populationCopy[i].fitness.values
            sum_fit_i_1 = populationCopy[i+1].fitness.values
            if sum_fit_i > sum_fit_i_1:
                populationCopy[i], populationCopy[i+1] = populationCopy[i+1], populationCopy[i]
                sign = True
        if not sign:
            break
    return populationCopy

def evaluate_offspring_in_parallel(offspring, taskflows, nodes, pset, work_processing):

    pool = multiprocessing.Pool()
    results = []

    for ind in offspring:
        tasks_copy = copy.deepcopy(taskflows)
        nodes_copy = copy.deepcopy(nodes)
        result = pool.apply_async(work_processing, (ind, tasks_copy, nodes_copy, pset,))
        results.append(result)

    pool.close()
    pool.join()

    fitnesses = [res.get() for res in results]
    return fitnesses

# def evaluate_on_testSets(individual, nodes, pset, pre_generated_taskflows):
#     results = []
#     for i,taskflows in enumerate(pre_generated_taskflows):
#         nodes_copy = copy.deepcopy(nodes)
#         taskflows = copy.deepcopy(taskflows)
#         fitness = work_processing(individual, taskflows, nodes_copy, pset)
#         results.append(fitness[0])
#     return results

def evaluate_on_testSets(individual, nodes, pset, pre_generated_taskflows, return_log=False):
    results = []
    logs = []

    for i, taskflows in enumerate(pre_generated_taskflows):
        nodes_copy = copy.deepcopy(nodes)
        taskflows_copy = copy.deepcopy(taskflows)

        if return_log:
            fitness, log_data = work_processing(individual, taskflows_copy, nodes_copy, pset, return_log=True)
            logs.append({
                "test_set_index": i,
                "fitness": fitness,
                "log": log_data
            })
        else:
            fitness = work_processing(individual, taskflows_copy, nodes_copy, pset)
        results.append(fitness[0])  # fitness is a tuple: (avg_time,)

    if return_log:
        return results, logs
    else:
        return results



def eaSimple(population, toolbox, taskflows, nodes, pre_generated_taskflows, num_TaskFlow, num_nodes,
             cxpb, mutpb, ngen, elitism, pset,
             min_fitness_values=None, genre_min_fitness_values=None,
             stats=None, halloffame=None, verbose=__debug__):
    if min_fitness_values is None:
        min_fitness_values = []
    if genre_min_fitness_values is None:
        genre_min_fitness_values = []

    logbook = tools.Logbook()
    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])

    # åˆå§‹ç§ç¾¤è¯„ä¼°ï¼ˆè¿™é‡Œä¹Ÿæ˜¯è¦å¯¹è®­ç»ƒé›†è¿›è¡Œæ”¹é€ çš„ï¼‰
    invalid_ind = [ind for ind in population if not ind.fitness.valid]
    fitnesses = evaluate_offspring_in_parallel(invalid_ind, taskflows, nodes, pset, work_processing)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit

    if halloffame is not None:
        halloffame.update(population)

    record = stats.compile(population) if stats else {}
    min_fitness_values.append(record["fitness"]["min"])
    logbook.record(gen=0, nevals=len(invalid_ind), **record)
    if verbose:
        print(logbook.stream)

    sorted_elite = sortPopulation(toolbox, population)[:elitism]

    # æµ‹è¯•é›†è¯„ä¼°æœ€ä¼˜ä¸ªä½“
    value_list = evaluate_on_testSets(sorted_elite[0], nodes, pset, pre_generated_taskflows)
    genre_min_fitness_values.append(sum(value_list) / NUM_TEST_SETS)

    # ä¿å­˜ç¬¬0ä»£æœ€ä¼˜ä¸ªä½“è°ƒåº¦æ—¥å¿—å’Œè¡¨è¾¾å¼
    taskflows_log = createTaskFlows(num_TaskFlow, 0, 0)
    nodes_log = copy.deepcopy(nodes)
    _, log_dict = work_processing(sorted_elite[0], taskflows_log, nodes_log, pset, return_log=True)
    os.makedirs("best_ind_logs", exist_ok=True)
    with open("best_ind_logs/gen_0_log.json", "w", encoding='utf-8') as f:
        json.dump(log_dict, f, indent=2, ensure_ascii=False)
    with open("best_ind_logs/gen_0_expr.txt", "w", encoding='utf-8') as f:
        f.write(str(sorted_elite[0]))

    for gen in range(1, ngen + 1):
        taskflows = createTaskFlows(num_TaskFlow, 0, gen)

        # é€‰æ‹© + å˜å¼‚ + äº¤å‰
        offspring = toolbox.select(population, len(population) - elitism)
        offspring = varAnd(offspring, toolbox, cxpb, mutpb)
        offspring[:] = sorted_elite + offspring

        # è®­ç»ƒé›†è¯„ä¼°
        fitnesses = evaluate_offspring_in_parallel(offspring, taskflows, nodes, pset, work_processing)
        for ind, fit in zip(offspring, fitnesses):
            ind.fitness.values = fit

        if halloffame is not None:
            halloffame.update(offspring)

        population[:] = offspring
        record = stats.compile(population) if stats else {}
        min_fitness_values.append(record["fitness"]["min"])
        logbook.record(gen=gen, nevals=len(invalid_ind), **record)

        if verbose:
            print(logbook.stream)

        sorted_elite = sortPopulation(toolbox, population)[:elitism]

        # æµ‹è¯•é›†è¯„ä¼°æœ€ä¼˜ä¸ªä½“
        value_list = evaluate_on_testSets(sorted_elite[0], nodes, pset, pre_generated_taskflows)
        genre_min_fitness_values.append(sum(value_list) / NUM_TEST_SETS)

        # ä¿å­˜å½“å‰ä»£æœ€ä¼˜ä¸ªä½“è°ƒåº¦æ—¥å¿—å’Œè¡¨è¾¾å¼
        taskflows_log = createTaskFlows(num_TaskFlow, 0, gen)
        nodes_log = copy.deepcopy(nodes)
        _, log_dict = work_processing(sorted_elite[0], taskflows_log, nodes_log, pset, return_log=True)
        with open(f"best_ind_logs/gen_{gen}_log.json", "w", encoding='utf-8') as f:
            json.dump(log_dict, f, indent=2, ensure_ascii=False)
        with open(f"best_ind_logs/gen_{gen}_expr.txt", "w", encoding='utf-8') as f:
            f.write(str(sorted_elite[0]))

    return population, logbook, genre_min_fitness_values, sorted_elite[0]


# def eaSimple(population, toolbox,taskflows,nodes,pre_generated_taskflows,num_TaskFlow,num_nodes, cxpb, mutpb, ngen, elitism,pset,min_fitness_values=[],genre_min_fitness_values=[],stats=None,
#              halloffame=None, verbose=__debug__, ):
#     logbook = tools.Logbook()
#     logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])
#
#
#     invalid_ind = [ind for ind in population if not ind.fitness.valid]
#
#     fitnesses = evaluate_offspring_in_parallel(invalid_ind, taskflows, nodes, pset, work_processing)
#
#     for ind, fit in zip(invalid_ind, fitnesses):
#         ind.fitness.values = fit
#     if halloffame is not None:
#         halloffame.update(population)
#
#     record = stats.compile(population) if stats else {}
#     min_fitness_values.append(record["fitness"]["min"])
#     logbook.record(gen=0, nevals=len(invalid_ind), **record)
#     if verbose:
#         print(logbook.stream)
#
#     sorted_elite = sortPopulation(toolbox, population)[:elitism]
#     # é€‚åº”åº¦è¯„ä¼°ï¼ˆéªŒè¯é›†ï¼‰
#     value_list = evaluate_on_testSets(sorted_elite[0], nodes, pset, pre_generated_taskflows)
#     genre_min_fitness_values.append(sum(value_list) / NUM_TEST_SETS)
#
#     # Begin the generational process
#     for gen in range(1, ngen + 1):
#         taskflows = createTaskFlows(num_TaskFlow,0,gen) #æ¯ä¸€ä»£éƒ½æ˜¯å…¨æ–°çš„taskflow
#
#         # Select the next generation individuals
#         offspring = toolbox.select(population, len(population)-elitism) # è¿™è¡Œä»£ç ä½¿ç”¨é€‰æ‹©æ“ä½œä»å½“å‰ç§ç¾¤ä¸­é€‰æ‹©ä¸åŸç§ç¾¤ç›¸åŒæ•°é‡çš„ä¸ªä½“ï¼Œè¿™äº›ä¸ªä½“æ„æˆæ–°çš„åä»£ã€‚,åªæ˜¯é€‰æ‹©çš„è¿‡ç¨‹ä¸º3é€‰ä¸€
#
#         # Vary the pool of individuals
#         offspring = varAnd(offspring, toolbox, cxpb, mutpb)
#
#         offspring[:] = sorted_elite + offspring
#
#         # é€‚åº”åº¦è¯„ä¼°ï¼ˆè®­ç»ƒé›†ï¼‰
#         fitnesses = evaluate_offspring_in_parallel(offspring, taskflows, nodes, pset, work_processing)
#
#         for ind, fit in zip(offspring, fitnesses):
#             ind.fitness.values = fit   #å°†è¿™äº›è®¡ç®—å‡ºçš„é€‚åº”åº¦å€¼èµ‹ç»™å¯¹åº”çš„ä¸ªä½“ï¼Œä½¿å¾—å®ƒä»¬çš„é€‚åº”åº¦å±æ€§å˜ä¸ºæœ‰æ•ˆã€‚
#
#         # Update the hall of fame with the generated individuals
#         if halloffame is not None:
#             halloffame.update(offspring)
#
#         # Replace the current population by the offspring
#         population[:] = offspring
#
#         # Append the current generation statistics to the logbook
#         record = stats.compile(population) if stats else {}
#         min_fitness_values.append(record["fitness"]["min"]) #æˆ‘åŠ çš„
#         logbook.record(gen=gen, nevals=len(invalid_ind), **record)
#
#         if verbose:
#             print(logbook.stream)
#
#         sorted_elite = sortPopulation(toolbox, population)[:elitism]
#
#
#         # é€‚åº”åº¦è¯„ä¼°ï¼ˆæµ‹è¯•é›†ï¼‰
#         value_list = evaluate_on_testSets(sorted_elite[0], nodes, pset, pre_generated_taskflows)
#         genre_min_fitness_values.append(sum(value_list) / NUM_TEST_SETS)
#     return population, logbook ,genre_min_fitness_values,sorted_elite[0]

def is_number(string):#åˆ›å»ºä¸€ä¸ªå‡½æ•° is_numberï¼Œç”¨äºæ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦å¯ä»¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚å¦‚æœå¯ä»¥ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    try:
        float(string)
        return True
    except ValueError:
        return False


# def plot_a_tree(tree):
#     red_nodes = []
#     purple_nodes = []
#     blue_nodes = []  # åˆ›å»ºä¸‰ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ä¸åŒé¢œè‰²èŠ‚ç‚¹çš„ç´¢å¼•ã€‚
#
#     for gid, g in enumerate(tree):
#         # å‡è®¾è¿™é‡Œçš„ blue_nodes æ˜¯æƒ³è¦æŠŠæ‰€æœ‰èŠ‚ç‚¹éƒ½æ ‡è®°ä¸ºè“è‰²
#         blue_nodes.append(gid)
#
#     # å‡è®¾ graph å‡½æ•°å·²ç»å®šä¹‰å¥½å¹¶è¿”å› nodes, edges, labels
#     nodes, edges, labels = graph(tree)
#
#     # ä¿®æ”¹ labels ä¸­çš„æ“ä½œç¬¦
#     for x in labels:
#         if labels[x] == "add":
#             labels[x] = "+"
#         elif labels[x] == "sub":
#             labels[x] = "-"
#         elif labels[x] == "mul":
#             labels[x] = "*"
#         elif labels[x] == "protected_div":
#             labels[x] = "/"
#
#     # åˆ›å»º NetworkX å›¾å¯¹è±¡
#     g = nx.Graph()
#     g.add_nodes_from(nodes)
#     g.add_edges_from(edges)
#
#     # è®¡ç®—æ¯ç§èŠ‚ç‚¹é¢œè‰²çš„ç´¢å¼•
#     red_nodes_idx = [nodes.index(n) for n in nodes if n in red_nodes]
#     purple_nodes_idx = [nodes.index(n) for n in nodes if n in purple_nodes]
#     blue_nodes_idx = [nodes.index(n) for n in nodes if n in blue_nodes]
#
#     # åˆ›å»ºæ–°å›¾å½¢å¯¹è±¡
#     plt.figure(figsize=(10, 6))  # æ¯æ¬¡ç»˜åˆ¶æ—¶åˆ›å»ºæ–°çš„å›¾å½¢å¯¹è±¡
#
#     # ä½¿ç”¨ graphviz_layout å¸ƒå±€ï¼Œä½¿èŠ‚ç‚¹æŒ‰æ ‘å½¢ç»“æ„æ’åˆ—
#     pos = graphviz_layout(g, prog="dot")
#
#     # ç»˜åˆ¶ä¸åŒé¢œè‰²çš„èŠ‚ç‚¹
#     nx.draw_networkx_nodes(g, pos, nodelist=red_nodes_idx, node_color="darkred", node_size=500, edgecolors='black', node_shape='o')
#     nx.draw_networkx_nodes(g, pos, nodelist=purple_nodes_idx, node_color="indigo", node_size=500, edgecolors='black', node_shape='o')
#     nx.draw_networkx_nodes(g, pos, nodelist=blue_nodes_idx, node_color="white", node_size=500, edgecolors='black', node_shape='o')
#
#     # ç»˜åˆ¶è¾¹
#     nx.draw_networkx_edges(g, pos, edge_color='black')
#
#     # ç»˜åˆ¶èŠ‚ç‚¹æ ‡ç­¾ï¼Œå­—ä½“é¢œè‰²ä¸ºé»‘è‰²
#     nx.draw_networkx_labels(g, pos, labels, font_color="black", font_size=9)  # å°†å­—ä½“é¢œè‰²è®¾ç½®ä¸ºé»‘è‰²ï¼Œå­—ä½“å¤§å°ä¸º9
#
#     # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
#     output_dir = r"D:\result\tree"
#     if not os.path.exists(output_dir):
#         os.makedirs(output_dir)
#
#     # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…è¦†ç›–
#     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
#     save_path = os.path.join(output_dir, f"tree_image_{timestamp}.pdf")
#
#     # ä¿å­˜å›¾åƒä¸ºPDFæ ¼å¼
#     plt.savefig(save_path, format='pdf', bbox_inches='tight')
#     plt.close()  # ç¡®ä¿æ¯æ¬¡éƒ½å…³é—­å½“å‰å›¾å½¢


def count_leaf_types(tree):
    leaf_count={}
    name_mapping = {
        'ARG0': 'a',
        'ARG1': 'b',
        'ARG2': 'c',
        'ARG3': 'd',
        'ARG4': 'e',
        'ARG5': 'f'
    }
    """ç»Ÿè®¡æ¯ç§å¶å­èŠ‚ç‚¹çš„æ•°é‡"""
    for x in tree:
        if isinstance(x, gp.Terminal):  # å¦‚æœæ˜¯å¶å­èŠ‚ç‚¹
            leaf_name = x.name
            if leaf_name in name_mapping:
                leaf_name = name_mapping[leaf_name]
            leaf_count[leaf_name] = leaf_count.get(leaf_name, 0) + 1
    return leaf_count


def protect_div(left, right):
    with np.errstate(divide='ignore', invalid='ignore'):
        x = np.divide(left, right)
        if isinstance(x, np.ndarray):
            x[np.isinf(x)] = 1
            x[np.isnan(x)] = 1
        elif np.isinf(x) or np.isnan(x):
            x = 1
    return x

def createTaskFlows(num_TaskFlow,genre,seed): #åˆ›å»ºå¤šä¸ªå·¥ä½œæµ #kä¸ºä»»åŠ¡æµéšæœºç§å­
    lambda_rate = 1  # å¹³å‡åˆ°è¾¾é€Ÿç‡ (Î»ï¼Œæ¯å•ä½æ—¶é—´å¹³å‡åˆ°è¾¾ä»»åŠ¡æ•°)
    np.random.seed(seed)
    interarrival_times = np.random.exponential(1 / lambda_rate, num_TaskFlow)  # ä¼šç”Ÿæˆä¸€ä¸ªåŒ…å« num_tasks ä¸ªä»»åŠ¡åˆ°è¾¾æ—¶é—´é—´éš”çš„æ•°ç»„

    # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„åˆ°è¾¾æ—¶é—´ï¼ˆé€šè¿‡ç´¯åŠ é—´éš”æ—¶é—´ï¼‰
    arrival_times = np.cumsum(interarrival_times)
    taskflows=[TaskFlow(id,arrival_time,genre,id+(seed)*num_TaskFlow) for id, arrival_time in zip(range(num_TaskFlow), arrival_times)]
    return taskflows

def present_time_update(present_time,taskflows):#åªæ›´æ–°é‚£äº›æœªå®Œæˆä»»åŠ¡çš„å½“å‰æ—¶é—´ï¼Œæ‰€ä»¥å½“ä¸€ä¸ªä»»åŠ¡å·²å®Œæˆæ—¶ï¼Œå®ƒçš„å½“å‰æ—¶é—´å°±æ˜¯å®Œæˆæ—¶é—´
    for taskflow in taskflows:
        for task in taskflow.tasks:
            if task.finish is False:
                task.present_time=present_time

def find_earlist_time(queue1,queue2):
    # ä½¿ç”¨ sort() æ–¹æ³•æ’åº
    queue1.sort(key=lambda x: x[1])
    queue2.sort(key=lambda x: x[1])
    task_queue1 = []
    task_queue2 = []
    if len(queue1)!=0 and len(queue2)!=0:
        min_time=min(queue1[0][1],queue2[0][1])
        for event in queue1:
            if event[1]==min_time:
                task_queue1.append(event[0])
            elif event[1] > min_time:
                break
        for event in queue2:
            if event[1]==min_time:
                task_queue2.append(event[0])
            elif event[1] > min_time:
                break
    elif len(queue1)==0  and len(queue2)!=0:
        min_time = queue2[0][1]
        for event in queue2:
            if event[1]==min_time:
                task_queue2.append(event[0])
            elif event[1] > min_time:
                break
    elif len(queue1)!=0  and len(queue2)==0:
        min_time = queue1[0][1]
        for event in queue1:
            if event[1]==min_time:
                task_queue1.append(event[0])
            elif event[1] > min_time:
                break
    return queue1,queue2,task_queue1,task_queue2


def decode1(individual, task, nodes, taskflows,pset):#decode1å‡½æ•°ä¸­éœ€è¦å¢åŠ ä¸€ä¸ªçº¦æŸæ¡ä»¶ï¼Œåˆ¤æ–­èŠ‚ç‚¹çš„ramï¼Œgpuï¼Œcpuå¤Ÿä¸å¤Ÿç”¨ï¼Œè¦åœ¨å¤Ÿç”¨çš„èŠ‚ç‚¹ä¸­é€‰æ‹©
    heuristic_1 = gp.compile(expr=individual[0], pset=pset[0])
    scores = []
    for node in nodes:
        heuristic_score = heuristic_1(computing_Task(task,node),task.present_time-task.arrivetime,len(task.descendant),computing_upload_time(task,node))
        scores.append((node, heuristic_score))
    best_node = max(scores, key=lambda x: x[1])[0]
    return best_node #æ‰¾åˆ°èŠ‚ç‚¹æœ¬èº«

def decode2(individual, node, taskflows,nodes, pset):
    heuristic_2 = gp.compile(expr=individual[1], pset=pset[1])
    scores = []
    for task in node.waiting_queue:
        k = task.taskflow_id#ä»»åŠ¡æ‰€åœ¨ä»»åŠ¡æµï¼Œåœ¨å¤šä»»åŠ¡æµä¸­çš„ä½ç½®
        heuristic_score = heuristic_2(computing_Task(task,node),task.present_time-task.arrivetime,len(task.descendant),
                                      0.1*computing_upload_time(task,node),taskflows[k].find_descendant_avg_time(taskflows,task,nodes)) #ä»»åŠ¡ç»™åç»§ä¼ é€’æ¶ˆæ¯çš„æ—¶é—´å–ä¸Šä¼ æ—¶é—´çš„0.1
        scores.append((task, heuristic_score))
    best_task = max(scores, key=lambda x: x[1])[0]
    return best_task #è¿”å›æ‰¾åˆ°çš„ä»»åŠ¡æœ¬èº«

def work_processing(individual, taskflows, nodes, pset, return_log=False):
    def sanitize_task_id(task):
        return getattr(task, "global_id", f"Task {task.id}")

    def sanitize_node_id(node):
        return f"N{node.id}({node.node_type})"

    task_execution_log = []
    node_assignment_log = {}
    taskflow_summary_log = []

    queue1 = []  # æœªæ‰§è¡Œä»»åŠ¡é˜Ÿåˆ—
    queue2 = []  # æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡é˜Ÿåˆ—
    present_time = 0
    present_time_update(present_time, taskflows)

    print("ğŸš€ [è°ƒåº¦å¼€å§‹] æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦æµç¨‹å¯åŠ¨...\n")

    for taskflow in taskflows:
        tasks = taskflow.find_predecessor_is_zero()
        for task in tasks:
            queue1.append((task, task.arrivetime))

    while queue1 or queue2:
        queue1, queue2, task_queue1, task_queue2 = find_earlist_time(queue1, queue2)

        if task_queue1:
            current_time = queue1[0][1]
            present_time_update(present_time=current_time, taskflows=taskflows)
            print(f"\nâ° æ—¶é—´æ¨è¿›è‡³ {current_time:.2f}ï¼Œå¤„ç†é˜Ÿåˆ— queue1 ä¸­çš„ä»»åŠ¡ï¼š")

            for task in task_queue1:
                node = decode1(individual, task, nodes, taskflows, pset)
                task.node = node
                print(f"ğŸŸ¡ ä»»åŠ¡ {sanitize_task_id(task)} åˆ†é…è‡³èŠ‚ç‚¹ {sanitize_node_id(node)}")

                if task.present_time >= node.begin_idle_time:
                    task_time = computing_Task(task, node) + computing_upload_time(task, node)
                    endtime = task.present_time + task_time
                    task.endtime = endtime
                    queue2.append((task, endtime))
                    node.begin_idle_time = endtime
                    print(f"âœ… æ‰§è¡Œä»»åŠ¡ {sanitize_task_id(task)}ï¼Œé¢„è®¡å®Œæˆæ—¶é—´ä¸º {endtime:.2f}")
                else:
                    node.waiting_queue.append(task)
                    print(f"â³ èŠ‚ç‚¹å¿™ï¼Œä»»åŠ¡ {sanitize_task_id(task)} åŠ å…¥ç­‰å¾…é˜Ÿåˆ—")

                task_execution_log.append({
                    "task_id": sanitize_task_id(task),
                    "taskflow_id": task.taskflow_id,
                    "node_id": sanitize_node_id(node),
                    "start_time": task.present_time,
                    "end_time": task.endtime if hasattr(task, 'endtime') else None
                })

                if sanitize_node_id(node) not in node_assignment_log:
                    node_assignment_log[sanitize_node_id(node)] = []
                node_assignment_log[sanitize_node_id(node)].append(sanitize_task_id(task))

                queue1 = [item for item in queue1 if item[0] != task]

        if task_queue2:
            current_time = queue2[0][1]
            present_time_update(present_time=current_time, taskflows=taskflows)
            print(f"\nğŸ æ—¶é—´æ¨è¿›è‡³ {current_time:.2f}ï¼Œå¤„ç†å®Œæˆçš„ä»»åŠ¡ï¼š")

            for finish_event in task_queue2:
                finish_event.finish = True
                print(f"âœ”ï¸ ä»»åŠ¡ {sanitize_task_id(finish_event)} æ‰§è¡Œå®Œæˆ")

            for task in task_queue2:
                taskflow = taskflows[task.taskflow_id]

                if task.descendant:
                    current_index = taskflow.tasks.index(task)
                    descendant_tasks = []
                    for d in task.descendant:
                        taskflow.tasks[d].predecessor.remove(current_index)
                        if len(taskflow.tasks[d].predecessor) == 0:
                            descendant_tasks.append(taskflow.tasks[d])
                    for descendant_task in descendant_tasks:
                        queue1.append((descendant_task, descendant_task.present_time))
                        print(f"â¡ï¸ åç»§ä»»åŠ¡ {sanitize_task_id(descendant_task)} æ‰€æœ‰å‰é©±å®Œæˆï¼ŒåŠ å…¥ queue1")
                else:
                    taskflow.finish_time = max(taskflow.finish_time, current_time)
                    print(f"ğŸ ä»»åŠ¡æµ {task.taskflow_id} æ›´æ–°å®Œæˆæ—¶é—´ä¸º {taskflow.finish_time:.2f}")

                if task.node.waiting_queue:
                    next_task = decode2(individual, task.node, taskflows, nodes, pset)
                    task.node.waiting_queue.remove(next_task)
                    trans_delay = 0.1 * computing_upload_time(task, task.node)
                    task_time = computing_Task(next_task, task.node) + computing_upload_time(next_task, task.node) + trans_delay
                    next_task.endtime = next_task.present_time + task_time
                    queue2.append((next_task, next_task.endtime))
                    task.node.begin_idle_time = next_task.endtime
                    print(f"ğŸ“¤ èŠ‚ç‚¹ {sanitize_node_id(task.node)} æ‰§è¡Œç­‰å¾…ä»»åŠ¡ {sanitize_task_id(next_task)}ï¼Œå®Œæˆæ—¶é—´ä¸º {next_task.endtime:.2f}")

                queue2 = [item for item in queue2 if item[0] != task]

    print("\nğŸ“¦ [ä»»åŠ¡åˆ†é…ç»Ÿè®¡] å„èŠ‚ç‚¹æ‰§è¡Œçš„ä»»åŠ¡åˆ†å¸ƒå¦‚ä¸‹ï¼š")
    for node in nodes:
        executed_tasks = node_assignment_log.get(sanitize_node_id(node), [])
        if executed_tasks:
            print(f"ğŸ“Œ èŠ‚ç‚¹ {sanitize_node_id(node)} æ‰§è¡Œä»»åŠ¡ï¼š{', '.join(executed_tasks)}")
        else:
            print(f"ğŸ“Œ èŠ‚ç‚¹ {sanitize_node_id(node)} æœªæ‰§è¡Œä»»ä½•ä»»åŠ¡")

    print("\nğŸ“Š [è°ƒåº¦å®Œæˆ] å¼€å§‹ç»Ÿè®¡ä»»åŠ¡æµå®Œæˆæ—¶é—´ï¼š")
    sum_time = 0
    count = 0
    for tf in taskflows[10:]:
        duration = tf.finish_time - tf.all_arrive_time
        taskflow_summary_log.append({
            "taskflow_id": tf.id,
            "start_time": tf.all_arrive_time,
            "end_time": tf.finish_time,
            "duration": duration
        })
        print(f"ğŸ“˜ TaskFlow {tf.id}ï¼šå¼€å§‹ {tf.all_arrive_time:.2f}ï¼Œå®Œæˆ {tf.finish_time:.2f}ï¼Œè€—æ—¶ {duration:.2f}")
        sum_time += duration
        count += 1

    avg_time = sum_time / count if count > 0 else 0
    print(f"\nğŸ“ˆ å 10 ä¸ªä»»åŠ¡æµçš„å¹³å‡å®Œæˆæ—¶é—´ä¸ºï¼š{avg_time:.2f}\n")

    if return_log:
        log_data = {
            "avg_time": avg_time,
            "task_execution_log": task_execution_log,
            "node_assignment_log": node_assignment_log,
            "taskflow_summary_log": taskflow_summary_log
        }
        return avg_time, log_data
    else:
        return (avg_time,)



def random_value():
    return random.randint(-1, 1)
# å®šä¹‰å‘½åå‡½æ•°æ¥æ›¿æ¢ lambda
def get_fitness_values(individual):
    return individual.fitness.values

def get_max_tree_height(individual):
    return max([tree.height for tree in individual])

def main_dual_tree():
    nodes = createNode(NUM_NODES,0)
    taskflows=createTaskFlows(NUM_TASKFLOWS,0,0)
    pre_generated_taskflows = []
    for i in range(NUM_TEST_SETS):
        taskflow = createTaskFlows(NUM_TASKFLOWS,1,i)
        pre_generated_taskflows.append(taskflow)
    pset = []
    for idx in range(2):
        if idx == 0:
            pset1 = gp.PrimitiveSet("MAIN", 4)
            pset1.addPrimitive(operator.add, 2)
            pset1.addPrimitive(operator.sub, 2)
            pset1.addPrimitive(operator.mul, 2)
            pset1.addPrimitive(protect_div, 2)
            #pset1.addPrimitive(np.divide, 2)
            pset1.addPrimitive(operator.neg, 1)
            pset1.addPrimitive(max, 2)
            pset1.addPrimitive(min, 2)
            #pset1.addEphemeralConstant("rand101", lambda: random.randint(-1, 1))

            pset1.renameArguments(ARG0='TCT')  # ä»»åŠ¡çš„è®¡ç®—æ—¶é—´ tct
            pset1.renameArguments(ARG1='TWT')  # ä»»åŠ¡çš„ç­‰å¾…æ—¶é—´=ä»»åŠ¡çš„åˆ°è¾¾æ—¶é—´-ä»»åŠ¡çš„å½“å‰æ—¶é—´ twt
            pset1.renameArguments(ARG2='TNC')  # ä»»åŠ¡çš„åç»§æ•°é‡ tnc
            #pset1.renameArguments(ARG3='ACT')  # åç»§èŠ‚ç‚¹çš„å¹³å‡å®Œæˆæ—¶é—´ act
            pset1.renameArguments(ARG3='TUT')  # ä»»åŠ¡çš„ä¸Šä¼ æ—¶é—´  tut


            pset.append(pset1)

        elif idx == 1:
            pset2 = gp.PrimitiveSet("MAIN", 5)
            pset2.addPrimitive(operator.add, 2)
            pset2.addPrimitive(operator.sub, 2)
            pset2.addPrimitive(operator.mul, 2)
            pset2.addPrimitive(protect_div, 2)
            #pset2.addPrimitive(np.divide, 2)
            pset2.addPrimitive(operator.neg, 1)
            pset2.addPrimitive(max, 2)
            pset2.addPrimitive(min, 2)

            pset2.renameArguments(ARG0='TCT')  # ä»»åŠ¡çš„è®¡ç®—æ—¶é—´
            pset2.renameArguments(ARG1='TWT')  # ä»»åŠ¡çš„ç­‰å¾…æ—¶é—´=ä»»åŠ¡çš„åˆ°è¾¾æ—¶é—´-ä»»åŠ¡çš„å½“å‰æ—¶é—´
            pset2.renameArguments(ARG2='TNC')  # ä»»åŠ¡çš„åç»§æ•°é‡
            #pset2.renameArguments(ARG3='NTQ')  # ç¬¬äºŒä¸ªæ ‘é˜Ÿåˆ—ä¸­ä»»åŠ¡ä¸ªæ•° ntq
            pset2.renameArguments(ARG3='TPS')  # ä»»åŠ¡çš„ç»™åç»§ä¼ é€’æ¶ˆæ¯çš„æ—¶é—´ tps
            pset2.renameArguments(ARG4='ACT')  # åç»§èŠ‚ç‚¹çš„å¹³å‡å®Œæˆæ—¶é—´ act
            pset.append(pset2)

    # åˆ›å»ºä¸€ä¸ªé€‚åº”åº¦ç±»å’Œä¸ªä½“ç±»ï¼Œä¸ªä½“ç”±å¤šæ£µæ ‘ç»„æˆ
    creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
    creator.create("Individual", list, fitness=creator.FitnessMin)

    toolbox = base.Toolbox()
    toolbox.register("expr", gp.genHalfAndHalf, min_=2, max_=4)
    toolbox.register("individual", initIndividual, creator.Individual, toolbox.expr, pset=pset, size=NUM_TREES)  # å‡è®¾æˆ‘ä»¬åˆ›å»º2ä¸ªæ ‘
    toolbox.register("population", tools.initRepeat, list, toolbox.individual)
    toolbox.register("evaluate", work_processing)#ä¸èƒ½ç»‘å®štaskflows=taskflows,nodes=nodes
    toolbox.register("select", tools.selTournament, tournsize=TOURNAMENT_SIZE)
    toolbox.register("mate", cxOnePointListOfTrees)
    toolbox.register("mutate", mutUniformListOfTrees, expr=toolbox.expr, pset=pset)
    toolbox.register("compile", gp.compile)

    population = toolbox.population(n=POP_SIZE)
    hof = tools.HallOfFame(1)
    stats_fit = tools.Statistics(key=get_fitness_values)
    stats_size = tools.Statistics(key=get_max_tree_height)
    mstats = tools.MultiStatistics(fitness=stats_fit, size=stats_size)
    mstats.register("avg", np.mean)
    mstats.register("std", np.std)
    mstats.register("min", np.min)
    mstats.register("max", np.max)
    pop, log ,genre_min_fitness_values,elite= eaSimple(population=population,
                                                       toolbox=toolbox,
                                                       taskflows=taskflows,
                                                       nodes=nodes,
                                                       pre_generated_taskflows=pre_generated_taskflows,
                                                       num_TaskFlow=NUM_TASKFLOWS,
                                                       num_nodes=NUM_NODES,
                                                       cxpb=CXPB,
                                                       mutpb=MUTPB,
                                                       ngen=NGEN,
                                                       elitism=ELITISM_NUM,
                                                       pset=pset,
                                                       min_fitness_values=[],
                                                       genre_min_fitness_values=[],
                                                       stats=mstats,
                                                       halloffame=hof,
                                                       verbose=True)

    # æŸ¥çœ‹æœ€ä½³ä¸ªä½“
    best_ind = hof[0]
    print('Best individual fitness:', best_ind.fitness)
    leaf_ratio_result=[]
    for tree in elite:
        # plot_a_tree(tree)

        # ç»Ÿè®¡æ¯ç§ç»ˆç«¯çš„æ•°é‡
        leaf_count = count_leaf_types(tree)

        # è®¡ç®—æ ‘ä¸­æ‰€æœ‰ç»ˆç«¯çš„æ€»æ•°
        total_leaves = sum(leaf_count.values())

        # è®¡ç®—æ¯ç§ç»ˆç«¯çš„æ¯”ä¾‹ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨OrderedDictä¸­
        leaf_ratio = OrderedDict((key, value / total_leaves) for key, value in leaf_count.items())
        leaf_ratio_result.append(leaf_ratio)
    return  genre_min_fitness_values,leaf_ratio_result


# def min_fitness_trend(min_fitness_values_1):
#     # åˆ›å»ºä¿å­˜è·¯å¾„çš„æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
#     save_folder = 'D:/result/picture'
#     if not os.path.exists(save_folder):
#         os.makedirs(save_folder)
#
#     # æ›´æ–°å­—ä½“å¤§å°
#     plt.rcParams.update({'font.size': 20})
#
#     # è®¾ç½®å›¾å½¢çš„å¤§å°
#     plt.figure(figsize=(10, 6))
#     plt.grid(True)
#
#     # æ¨ªåæ ‡ä¸ºæ¬¡æ•°ï¼Œå‡è®¾ä¸ºåˆ—è¡¨çš„ç´¢å¼•
#     x = list(range(len(min_fitness_values_1)))  # æ¨ªåæ ‡
#
#     # ç»˜åˆ¶å„æ¡æ›²çº¿
#     plt.plot(x, min_fitness_values_1, label='DTGP', marker='o')
#
#     # æ·»åŠ æ ‡ç­¾å’Œæ ‡é¢˜
#     plt.xlabel('Generation')
#     plt.ylabel('Makespan(ms)')
#
#     # è°ƒæ•´å›¾ä¾‹çš„ä½ç½®ï¼Œé˜²æ­¢é®æŒ¡
#     plt.legend(loc='upper right')
#
#     # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…è¦†ç›–
#     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
#     save_path = os.path.join(save_folder, f"fitness_plot_{timestamp}.pdf")
#
#     # ä¿å­˜å›¾åƒï¼Œå¹¶è£å‰ªæ‰å‘¨å›´çš„ç©ºç™½åŒºåŸŸ
#     plt.savefig(save_path, format='pdf', bbox_inches='tight')
#     plt.close()  # å…³é—­å›¾å½¢ï¼Œé˜²æ­¢å›¾å½¢åœ¨æ˜¾ç¤ºæ—¶è¢«å¤šæ¬¡è¦†ç›–


def keyadd(p,x):
    all_keys = set(p.keys()).union(x.keys())
    result = OrderedDict()
    for key in all_keys:
        result[key] = p.get(key, 0) + x.get(key, 0)
    return result

if __name__ == '__main__':

    # genre_min_fitness_values_sum=[526, 440, 450, 437, 429 ,442, 421.2 ,422, 410 ,412 ,406 ,381.6 ,392 ,375, 367 ,377.2, 379.3 ,367 ,369, 372, 358, 357 ,358 ,361,367.5 ,358, 362 ,368 ,360.4 ,361 ,363, 358.6 ,373, 368.1, 368.4 ,360 ,368 ,375 ,366 ,356, 367, 362 ,358.6 ,366.6 ,356, 357.3, 357, 356, 354]
    # æ‰€æœ‰è¿è¡Œçš„ç»“æœæ±‡æ€»
    run_fitness_history = []
    # è®°å½•æ¯ä¸€ä»£çš„æµ‹è¯•é›†æœ€å°é€‚åº”åº¦çš„ç´¯åŠ å€¼
    genre_min_fitness_values_sum = [0] * NGEN
    # è®°å½•æ¯æ£µæ ‘çš„å¶å­ç±»å‹ç»Ÿè®¡æ¯”ä¾‹ç´¯åŠ å€¼
    leaf_ratio_result_sum = [OrderedDict() for _ in range(NUM_TREES)]
    # NUM_RUNS æ¬¡ç‹¬ç«‹è¿è¡Œ
    for _ in range(NUM_RUNS):
        # æ¯æ¬¡ç‹¬ç«‹è¿è¡Œåï¼Œå¾—åˆ°æ¯ä¸€ä»£çš„æµ‹è¯•é›†çš„æœ€å°é€‚åº”åº¦ï¼Œä»¥åŠæ¯æ£µæ ‘çš„å¶å­æ¯”ä¾‹ç»Ÿè®¡
        genre_min_fitness_values, leaf_ratio_result = main_dual_tree()

        run_fitness_history.append(genre_min_fitness_values)
        genre_min_fitness_values_sum = [a + b for a, b in
                                        zip(genre_min_fitness_values, genre_min_fitness_values_sum)]
        leaf_ratio_result_sum = [keyadd(a, b) for a, b in zip(leaf_ratio_result, leaf_ratio_result_sum)]

    genre_min_fitness_values_sum = [a / NUM_RUNS for a in genre_min_fitness_values_sum]

    print(leaf_ratio_result_sum)
    print(run_fitness_history)

    # min_fitness_trend(genre_min_fitness_values_sum)



