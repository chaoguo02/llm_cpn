import copy
import multiprocessing

from core_function.update_time import (
    present_time_update, find_earlist_time,
    computing_Task, computing_upload_time
)

def evaluate_offspring_in_parallel(offspring, taskflows, nodes, pset,decode1,decode2, work_processing):
    pool = multiprocessing.Pool()
    results = []

    for ind in offspring:
        tasks_copy = copy.deepcopy(taskflows)
        nodes_copy = copy.deepcopy(nodes)
        result = pool.apply_async(work_processing, (ind, tasks_copy, nodes_copy, pset,decode1,decode2))
        results.append(result)

    pool.close()
    pool.join()

    fitnesses = [res.get() for res in results]
    return fitnesses


def evaluate_on_testSets(individual, nodes, pset,decode1,decode2, pre_generated_taskflows, return_log=False):
    results = []
    logs = []

    for i, taskflows in enumerate(pre_generated_taskflows):
        nodes_copy = copy.deepcopy(nodes)
        taskflows_copy = copy.deepcopy(taskflows)

        if return_log:
            fitness, log_data = work_processing(individual, taskflows_copy, nodes_copy, pset,decode1,decode2, return_log=True)
            logs.append({
                "test_set_index": i,
                "fitness": fitness,
                "log": log_data
            })
        else:
            fitness = work_processing(individual, taskflows_copy, nodes_copy, pset,decode1, decode2)
        results.append(fitness[0])

    return (results, logs) if return_log else results


def work_processing(individual, taskflows, nodes, pset,decode1, decode2, return_log=False):
    def sanitize_task_id(task):
        return getattr(task, "global_id", f"Task {task.id}")

    def sanitize_node_id(node):
        return f"N{node.id}({node.node_type})"

    task_execution_log = []
    node_assignment_log = {}
    taskflow_summary_log = []
    skipped_tasks_log = []

    queue1 = []
    queue2 = []
    present_time = 0
    present_time_update(present_time, taskflows)
    print("\nğŸš€ [è°ƒåº¦å¼€å§‹] æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦æµç¨‹å¯åŠ¨...")

    for taskflow in taskflows:
        tasks = taskflow.find_predecessor_is_zero()
        for task in tasks:
            queue1.append((task, task.arrivetime))

    while queue1 or queue2:
        queue1, queue2, task_queue1, task_queue2 = find_earlist_time(queue1, queue2)

        if task_queue1:
            current_time = queue1[0][1]
            present_time_update(present_time=current_time, taskflows=taskflows)
            print(f"\nâ° æ—¶é—´æ¨è¿›è‡³ {current_time:.2f}ï¼Œå¤„ç† queue1 ä¸­çš„ä»»åŠ¡ï¼š")

            for task in task_queue1:
                node = decode1(individual, task, nodes, taskflows, pset)
                task.node = node
                print(f"ğŸŸ¡ ä»»åŠ¡ {sanitize_task_id(task)} åˆ†é…è‡³èŠ‚ç‚¹ {sanitize_node_id(node)}")

                # æ‰§è¡Œè¿™ä¸ªtaskäº†ï¼Œå°±å°†nodeçš„begin_idle_timeè®¾ç½®ä¸ºè¿™ä¸ªä»»åŠ¡çš„ç»“æŸæ—¶é—´
                if task.present_time >= node.begin_idle_time:
                    task_time = computing_Task(task, node) + computing_upload_time(task, node)
                    endtime = task.present_time + task_time
                    task.endtime = endtime
                    queue2.append((task, endtime))
                    node.begin_idle_time = endtime

                    task_execution_log.append({
                        "task_id": sanitize_task_id(task),
                        "taskflow_id": task.taskflow_id,
                        "node_id": sanitize_node_id(node),
                        "start_time": task.present_time,
                        "end_time": task.endtime
                    })

                    print(f"âœ… æ‰§è¡Œä»»åŠ¡ {sanitize_task_id(task)}ï¼Œå®Œæˆæ—¶é—´ {endtime:.2f}")
                # å¦‚æœå½“å‰è¯¥èŠ‚ç‚¹æœ‰ä»»åŠ¡åœ¨æ‰§è¡Œï¼Œåœ¨FIFOä¸­ï¼Œç­‰è¿™ä¸ªä»»åŠ¡æ‰§è¡Œå®Œäº†å°±åº”è¯¥ç»§ç»­æ‰§è¡Œæ–°ä»»åŠ¡äº†ï¼Œ
                # ä½†æ˜¯åœ¨gpä¸­ï¼Œè¿™ä¸ªä»»åŠ¡éœ€è¦åŠ å…¥nodeçš„ç­‰å¾…é˜Ÿåˆ—ä¸­
                else:
                    node.waiting_queue.append(task)
                    print(f"â³ èŠ‚ç‚¹å¿™ï¼Œä»»åŠ¡ {sanitize_task_id(task)} åŠ å…¥ç­‰å¾…é˜Ÿåˆ—")

                if sanitize_node_id(node) not in node_assignment_log:
                    node_assignment_log[sanitize_node_id(node)] = []
                node_assignment_log[sanitize_node_id(node)].append(sanitize_task_id(task))

                queue1 = [item for item in queue1 if item[0] != task]

        if task_queue2:
            current_time = queue2[0][1]
            present_time_update(present_time=current_time, taskflows=taskflows)
            print(f"\nğŸ æ—¶é—´æ¨è¿›è‡³ {current_time:.2f}ï¼Œå¤„ç†å®Œæˆä»»åŠ¡ï¼š")

            for finish_event in task_queue2:
                finish_event.finish = True
                print(f"âœ”ï¸ ä»»åŠ¡ {sanitize_task_id(finish_event)} æ‰§è¡Œå®Œæˆ")

            for task in task_queue2:
                taskflow = taskflows[task.taskflow_id]
                if task.descendant:
                    current_index = taskflow.tasks.index(task)
                    descendant_tasks = []
                    for d in task.descendant:
                        taskflow.tasks[d].predecessor.remove(current_index)
                        if not taskflow.tasks[d].predecessor:
                            descendant_tasks.append(taskflow.tasks[d])
                    for descendant_task in descendant_tasks:
                        queue1.append((descendant_task, descendant_task.present_time))
                        print(f"â¡ï¸ åç»§ä»»åŠ¡ {sanitize_task_id(descendant_task)} åŠ å…¥ queue1")
                else:
                    taskflow.finish_time = max(taskflow.finish_time, current_time)
                    print(f"ğŸ ä»»åŠ¡æµ {task.taskflow_id} å®Œæˆæ—¶é—´æ›´æ–°ä¸º {taskflow.finish_time:.2f}")

                if task.node.waiting_queue:
                    next_task = decode2(individual, task.node, taskflows, nodes, pset)
                    if next_task is None:
                        continue
                    task.node.waiting_queue.remove(next_task)
                    trans_delay = 0.1 * computing_upload_time(task, task.node)
                    task_time = computing_Task(next_task, task.node) + computing_upload_time(next_task, task.node) + trans_delay
                    next_task.endtime = next_task.present_time + task_time
                    queue2.append((next_task, next_task.endtime))
                    task.node.begin_idle_time = next_task.endtime

                    task_execution_log.append({
                        "task_id": sanitize_task_id(next_task),
                        "taskflow_id": next_task.taskflow_id,
                        "node_id": sanitize_node_id(task.node),
                        "start_time": next_task.present_time,
                        "end_time": next_task.endtime
                    })
                    print(f"ğŸ“¤ èŠ‚ç‚¹ {sanitize_node_id(task.node)} æ‰§è¡Œç­‰å¾…ä»»åŠ¡ {sanitize_task_id(next_task)}ï¼Œå®Œæˆæ—¶é—´ {next_task.endtime:.2f}")

                queue2 = [item for item in queue2 if item[0] != task]

    print("\nğŸ“¦ èŠ‚ç‚¹ä»»åŠ¡åˆ†å¸ƒï¼š")
    for node in nodes:
        executed_tasks = node_assignment_log.get(sanitize_node_id(node), [])
        if executed_tasks:
            print(f"ğŸ“Œ {sanitize_node_id(node)} æ‰§è¡Œä»»åŠ¡ï¼š{', '.join(executed_tasks)}")
        else:
            print(f"ğŸ“Œ {sanitize_node_id(node)} æœªæ‰§è¡Œä»»ä½•ä»»åŠ¡")

    print("\nğŸ“Š å¼€å§‹ç»Ÿè®¡ä»»åŠ¡æµå®Œæˆæ—¶é—´ï¼š")
    sum_time = 0
    count = 0
    for tf in taskflows[10:]:
        duration = tf.finish_time - tf.all_arrive_time
        taskflow_summary_log.append({
            "taskflow_id": tf.id,
            "start_time": tf.all_arrive_time,
            "end_time": tf.finish_time,
            "duration": duration
        })
        print(f"ğŸ“˜ TaskFlow {tf.id}ï¼šè€—æ—¶ {duration:.2f}")
        sum_time += duration
        count += 1

    avg_time = sum_time / count if count else 0
    print(f"\nğŸ“ˆ å 10 ä¸ªä»»åŠ¡æµå¹³å‡å®Œæˆæ—¶é—´ï¼š{avg_time:.2f}")

    if return_log:
        log_data = {
            "avg_time": avg_time,
            "task_execution_log": task_execution_log,
            "node_assignment_log": node_assignment_log,
            "taskflow_summary_log": taskflow_summary_log,
            "skipped_tasks_log": skipped_tasks_log
        }
        return avg_time, log_data
    return (avg_time,)



# def allocate_resources(node, task):
#     node.cpu_capacity -= task.cpu_require
#     node.ram_capacity -= task.ram_require
#     node.gpu_capacity -= task.gpu_require
#
# def release_resources(node, task):
#     node.cpu_capacity += task.cpu_require
#     node.ram_capacity += task.ram_require
#     node.gpu_capacity += task.gpu_require