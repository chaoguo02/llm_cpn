import copy
import multiprocessing

from core_function.update_time import present_time_update, find_earlist_time, computing_Task, computing_upload_time

def evaluate_offspring_in_parallel(offspring, taskflows, nodes, pset, work_processing):
    pool = multiprocessing.Pool()
    results = []

    for ind in offspring:
        tasks_copy = copy.deepcopy(taskflows)
        nodes_copy = copy.deepcopy(nodes)
        result = pool.apply_async(work_processing, (ind, tasks_copy, nodes_copy, pset,))
        results.append(result)

    pool.close()
    pool.join()

    fitnesses = [res.get() for res in results]
    return fitnesses

def evaluate_on_testSets(individual, nodes, pset, pre_generated_taskflows, return_log=False):
    results = []
    logs = []

    for i, taskflows in enumerate(pre_generated_taskflows):
        nodes_copy = copy.deepcopy(nodes)
        taskflows_copy = copy.deepcopy(taskflows)

        if return_log:
            fitness, log_data = work_processing(individual, taskflows_copy, nodes_copy, pset, return_log=True)
            logs.append({
                "test_set_index": i,
                "fitness": fitness,
                "log": log_data
            })
        else:
            fitness = work_processing(individual, taskflows_copy, nodes_copy, pset)
        results.append(fitness[0])  # fitness is a tuple: (avg_time,)

    if return_log:
        return results, logs
    else:
        return results


def work_processing(individual, taskflows, nodes, pset, return_log=False):
    def sanitize_task_id(task):
        return getattr(task, "global_id", f"Task {task.id}")

    def sanitize_node_id(node):
        return f"N{node.id}({node.node_type})"

    task_execution_log = []
    node_assignment_log = {}
    taskflow_summary_log = []
    skipped_tasks_log = []

    queue1 = []  # æœªæ‰§è¡Œä»»åŠ¡é˜Ÿåˆ—
    queue2 = []  # æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡é˜Ÿåˆ—
    present_time = 0
    present_time_update(present_time, taskflows)

    print("ğŸš€ [è°ƒåº¦å¼€å§‹] æ¨¡æ‹Ÿä»»åŠ¡è°ƒåº¦æµç¨‹å¯åŠ¨...\n")

    for taskflow in taskflows:
        tasks = taskflow.find_predecessor_is_zero()
        for task in tasks:
            queue1.append((task, task.arrivetime))

    while queue1 or queue2:
        queue1, queue2, task_queue1, task_queue2 = find_earlist_time(queue1, queue2)

        if task_queue1:
            current_time = queue1[0][1]
            present_time_update(present_time=current_time, taskflows=taskflows)
            print(f"\nâ° æ—¶é—´æ¨è¿›è‡³ {current_time:.2f}ï¼Œå¤„ç†é˜Ÿåˆ— queue1 ä¸­çš„ä»»åŠ¡ï¼š")

            for task in task_queue1:
                node = decode1(individual, task, nodes, taskflows, pset)
                task.node = node
                print(f"ğŸŸ¡ ä»»åŠ¡ {sanitize_task_id(task)} åˆ†é…è‡³èŠ‚ç‚¹ {sanitize_node_id(node)}")

                if task.present_time >= node.begin_idle_time:
                    try:
                        allocate_resources(task, node)
                    except Exception as e:
                        print(f"[âŒèµ„æºåˆ†é…å¤±è´¥] Task {task.global_id} åˆ†é…åˆ° Node {node.id} æ—¶å¤±è´¥ï¼š{e}")
                        skipped_tasks_log.append({
                            "task_id": sanitize_task_id(task),
                            "taskflow_id": task.taskflow_id,
                            "reason": f"èµ„æºåˆ†é…å¤±è´¥: {str(e)}",
                            "node_id": sanitize_node_id(node),
                            "present_time": task.present_time
                        })
                        continue

                    task_time = computing_Task(task, node) + computing_upload_time(task, node)
                    endtime = task.present_time + task_time
                    task.endtime = endtime
                    queue2.append((task, endtime))
                    node.begin_idle_time = endtime

                    # âœ… è®°å½•æˆåŠŸè°ƒåº¦çš„ä»»åŠ¡
                    task_execution_log.append({
                        "task_id": sanitize_task_id(task),
                        "taskflow_id": task.taskflow_id,
                        "node_id": sanitize_node_id(node),
                        "start_time": task.present_time,
                        "end_time": task.endtime
                    })

                    print(f"âœ… æ‰§è¡Œä»»åŠ¡ {sanitize_task_id(task)}ï¼Œé¢„è®¡å®Œæˆæ—¶é—´ä¸º {endtime:.2f}")
                else:
                    node.waiting_queue.append(task)
                    print(f"â³ èŠ‚ç‚¹å¿™ï¼Œä»»åŠ¡ {sanitize_task_id(task)} åŠ å…¥ç­‰å¾…é˜Ÿåˆ—")

                if sanitize_node_id(node) not in node_assignment_log:
                    node_assignment_log[sanitize_node_id(node)] = []
                node_assignment_log[sanitize_node_id(node)].append(sanitize_task_id(task))

                queue1 = [item for item in queue1 if item[0] != task]

        if task_queue2:
            current_time = queue2[0][1]
            present_time_update(present_time=current_time, taskflows=taskflows)
            print(f"\nğŸ æ—¶é—´æ¨è¿›è‡³ {current_time:.2f}ï¼Œå¤„ç†å®Œæˆçš„ä»»åŠ¡ï¼š")

            for finish_event in task_queue2:
                finish_event.finish = True
                release_resources(finish_event, finish_event.node)
                print(f"âœ”ï¸ ä»»åŠ¡ {sanitize_task_id(finish_event)} æ‰§è¡Œå®Œæˆ")

            for task in task_queue2:
                taskflow = taskflows[task.taskflow_id]

                if task.descendant:
                    current_index = taskflow.tasks.index(task)
                    descendant_tasks = []
                    for d in task.descendant:
                        taskflow.tasks[d].predecessor.remove(current_index)
                        if len(taskflow.tasks[d].predecessor) == 0:
                            descendant_tasks.append(taskflow.tasks[d])
                    for descendant_task in descendant_tasks:
                        queue1.append((descendant_task, descendant_task.present_time))
                        print(f"â¡ï¸ åç»§ä»»åŠ¡ {sanitize_task_id(descendant_task)} æ‰€æœ‰å‰é©±å®Œæˆï¼ŒåŠ å…¥ queue1")
                else:
                    taskflow.finish_time = max(taskflow.finish_time, current_time)
                    print(f"ğŸ ä»»åŠ¡æµ {task.taskflow_id} æ›´æ–°å®Œæˆæ—¶é—´ä¸º {taskflow.finish_time:.2f}")

                if task.node.waiting_queue:
                    next_task = decode2(individual, task.node, taskflows, nodes, pset)
                    task.node.waiting_queue.remove(next_task)
                    trans_delay = 0.1 * computing_upload_time(task, task.node)
                    task_time = computing_Task(next_task, task.node) + computing_upload_time(next_task, task.node) + trans_delay
                    next_task.endtime = next_task.present_time + task_time
                    queue2.append((next_task, next_task.endtime))
                    task.node.begin_idle_time = next_task.endtime

                    # âœ… è®°å½• decode2 åˆ†é…çš„ä»»åŠ¡
                    task_execution_log.append({
                        "task_id": sanitize_task_id(next_task),
                        "taskflow_id": next_task.taskflow_id,
                        "node_id": sanitize_node_id(task.node),
                        "start_time": next_task.present_time,
                        "end_time": next_task.endtime
                    })

                    print(f"ğŸ“¤ èŠ‚ç‚¹ {sanitize_node_id(task.node)} æ‰§è¡Œç­‰å¾…ä»»åŠ¡ {sanitize_task_id(next_task)}ï¼Œå®Œæˆæ—¶é—´ä¸º {next_task.endtime:.2f}")

                queue2 = [item for item in queue2 if item[0] != task]

    print("\nğŸ“¦ [ä»»åŠ¡åˆ†é…ç»Ÿè®¡] å„èŠ‚ç‚¹æ‰§è¡Œçš„ä»»åŠ¡åˆ†å¸ƒå¦‚ä¸‹ï¼š")
    for node in nodes:
        executed_tasks = node_assignment_log.get(sanitize_node_id(node), [])
        if executed_tasks:
            print(f"ğŸ“Œ èŠ‚ç‚¹ {sanitize_node_id(node)} æ‰§è¡Œä»»åŠ¡ï¼š{', '.join(executed_tasks)}")
        else:
            print(f"ğŸ“Œ èŠ‚ç‚¹ {sanitize_node_id(node)} æœªæ‰§è¡Œä»»ä½•ä»»åŠ¡")

    print("\nğŸ“Š [è°ƒåº¦å®Œæˆ] å¼€å§‹ç»Ÿè®¡ä»»åŠ¡æµå®Œæˆæ—¶é—´ï¼š")
    sum_time = 0
    count = 0
    for tf in taskflows[10:]:
        duration = tf.finish_time - tf.all_arrive_time
        taskflow_summary_log.append({
            "taskflow_id": tf.id,
            "start_time": tf.all_arrive_time,
            "end_time": tf.finish_time,
            "duration": duration
        })
        print(f"ğŸ“˜ TaskFlow {tf.id}ï¼šå¼€å§‹ {tf.all_arrive_time:.2f}ï¼Œå®Œæˆ {tf.finish_time:.2f}ï¼Œè€—æ—¶ {duration:.2f}")
        sum_time += duration
        count += 1

    avg_time = sum_time / count if count > 0 else 0
    print(f"\nğŸ“ˆ å 10 ä¸ªä»»åŠ¡æµçš„å¹³å‡å®Œæˆæ—¶é—´ä¸ºï¼š{avg_time:.2f}\n")

    if return_log:
        log_data = {
            "avg_time": avg_time,
            "task_execution_log": task_execution_log,
            "node_assignment_log": node_assignment_log,
            "taskflow_summary_log": taskflow_summary_log,
            "skipped_tasks_log": skipped_tasks_log
        }
        return avg_time, log_data
    else:
        return (avg_time,)


def allocate_resources(node, task):
    node.cpu_capacity -= task.cpu_require
    node.ram_capacity -= task.ram_require
    node.gpu_capacity -= task.gpu_require

def release_resources(node, task):
    node.cpu_capacity += task.cpu_require
    node.ram_capacity += task.ram_require
    node.gpu_capacity += task.gpu_require